import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import yaml
import os
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
import shutil


class DefectDataset(Dataset):
    """ì–‘í’ˆ/ë¶ˆëŸ‰ ë¶„ë¥˜ ë°ì´í„°ì…‹"""
    def __init__(self, txt_file, transform=None, label_map=None):
        """
        Args:
            txt_file (str): ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ì´ ë‹´ê¸´ txt íŒŒì¼
                           í˜•ì‹: /path/to/image.jpg 0 (ë˜ëŠ” 1)
            transform: ì´ë¯¸ì§€ ë³€í™˜
        """
        self.data = []
        self.transform = transform
        self.label_map = label_map
        
        with open(txt_file, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split()
                if len(parts) >= 2:
                    img_path = ' '.join(parts[:-1])  # ê²½ë¡œì— ê³µë°±ì´ ìˆì„ ìˆ˜ ìˆìŒ
                    label = int(parts[-1])
                    self.data.append((img_path, label))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        img_path, label = self.data[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
            # ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
            image = Image.new('RGB', (224, 224), color='black')
        
        # label mapping (ì˜ˆ: door+simple: 1,2,3 -> 1)
        if self.label_map is not None:
            label = self.label_map.get(label, label)

        if self.transform:
            image = self.transform(image)
        
        return image, label


class DINOv2Trainer:
    def __init__(self, project_name='dinov2_training', 
                 model_size='small', imgsz=224, batch_size=32, 
                 epochs=100, lr=1e-4, device='cuda', project_dir='runs',
                 lr_backbone=None, lr_head=None, freeze_epochs=0):
        """
        DINOv2 ì–‘í’ˆ/ë¶ˆëŸ‰ ë¶„ë¥˜ í•™ìŠµ í´ë˜ìŠ¤
        
        Args:
            project_name (str): í”„ë¡œì íŠ¸ ì´ë¦„
            model_size (str): ëª¨ë¸ í¬ê¸° ('small', 'base', 'large', 'giant')
            imgsz (int): ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 224)
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜
            lr (float): í•™ìŠµë¥ 
            device (str): ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
            project_dir (str): í”„ë¡œì íŠ¸ í´ë” (ê¸°ë³¸ê°’: 'runs')
            lr_backbone (float): ë°±ë³¸ í•™ìŠµë¥ 
            lr_head (float): í—¤ë“œ í•™ìŠµë¥ 
            freeze_epochs (int): ë°±ë³¸ ê³ ì • ì—í¬í¬ ìˆ˜
        """
        self.project_name = project_name
        self.model_size = model_size
        self.imgsz = imgsz
        self.batch_size = batch_size
        self.epochs = epochs
        self.lr = lr
        # ë¶„ë¦¬ í•™ìŠµë¥  ë° ì´ˆê¸° ë°±ë³¸ ê³ ì • ì„¤ì •
        self.lr_head = lr_head if lr_head is not None else lr
        self.lr_backbone = lr_backbone if lr_backbone is not None else max(lr * 0.1, 1e-6)
        self.freeze_epochs = max(int(freeze_epochs), 0)
        self.device = device if torch.cuda.is_available() else 'cpu'
        
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.run_name = f"{self.project_name}_{self.timestamp}"
        self.save_dir = Path(project_dir) / self.run_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # í´ë˜ìŠ¤ ì •ë³´ (YAMLì˜ parts/modeì— ë”°ë¼ ì„¤ì •)
        self.class_names = None
        self.num_classes = None
        self.parts = None
        self.mode = None
        self.label_map = None  # ex) door+simple: {0:0,1:1,2:1,3:1}
        self.preprocess_enabled = True  # YAMLì˜ preprocess on/off
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = None
        self._opt_group_head_idx = None
        self._opt_group_backbone_idx = None
        
        # AMP (autocast + GradScaler) ê¸°ë³¸ í™œì„±í™”
        self.use_amp = True
        self.scaler = torch.cuda.amp.GradScaler(enabled=(self.device == 'cuda'))
        
        # í•™ìŠµ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        
        print(f"âœ“ DINOv2 Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - í”„ë¡œì íŠ¸: {self.project_name}")
        print(f"  - ëª¨ë¸ í¬ê¸°: {self.model_size}")
        print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {self.imgsz}")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"  - ì—í¬í¬: {self.epochs}")
        print(f"  - í•™ìŠµë¥ : {self.lr}")
        print(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  - ì €ì¥ ìœ„ì¹˜: {self.save_dir}")
    
    def _load_dinov2_model(self):
        """DINOv2 ë°±ë³¸ ë¡œë“œ ë° ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€"""
        print(f"\nğŸ”„ DINOv2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # DINOv2 ëª¨ë¸ ë§¤í•‘
        model_map = {
            'small': 'dinov2_vits14',
            'base': 'dinov2_vitb14',
            'large': 'dinov2_vitl14',
            'giant': 'dinov2_vitg14'
        }
        
        model_name = model_map.get(self.model_size, 'dinov2_vits14')
        
        try:
            # torch hubì—ì„œ DINOv2 ë¡œë“œ
            backbone = torch.hub.load('facebookresearch/dinov2', model_name)
            
            # íŠ¹ì§• ì°¨ì› ê°€ì ¸ì˜¤ê¸°
            if self.model_size == 'small':
                embed_dim = 384
            elif self.model_size == 'base':
                embed_dim = 768
            elif self.model_size == 'large':
                embed_dim = 1024
            elif self.model_size == 'giant':
                embed_dim = 1536
            else:
                embed_dim = 384
            
            # ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€
            class DINOv2Classifier(nn.Module):
                def __init__(self, backbone, embed_dim, num_classes=2):
                    super().__init__()
                    self.backbone = backbone
                    self.classifier = nn.Sequential(
                        nn.Linear(embed_dim, 512),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(512, num_classes)
                    )
                
                def forward(self, x):
                    # DINOv2ëŠ” CLS í† í°ì„ ë°˜í™˜
                    features = self.backbone(x)
                    return self.classifier(features)
            
            # num_classesëŠ” YAML ë¡œë“œ í›„ ì„¤ì •ë¨
            num_classes = self.num_classes if self.num_classes is not None else 2
            self.model = DINOv2Classifier(backbone, embed_dim, num_classes=num_classes)
            self.model = self.model.to(self.device)

            # Optimizer ì„¤ì •: ë°±ë³¸/í—¤ë“œ íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬
            head_params = list(self.model.classifier.parameters())
            backbone_params = list(self.model.backbone.parameters())

            self.optimizer = optim.AdamW([
                {'params': head_params, 'lr': self.lr_head},
                {'params': backbone_params, 'lr': 0.0 if self.freeze_epochs > 0 else self.lr_backbone},
            ], weight_decay=0.01)

            # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì¸ë±ìŠ¤ ê¸°ë¡
            self._opt_group_head_idx = 0
            self._opt_group_backbone_idx = 1
            
            print(f"âœ“ {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (íŠ¹ì§• ì°¨ì›: {embed_dim})")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"ğŸ’¡ torchì™€ torchvisionì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            raise
    
    def load_data_yaml(self, yaml_path):
        """
        ë°ì´í„° YAML íŒŒì¼ ë¡œë“œ
        
        Args:
            yaml_path (str): YAML íŒŒì¼ ê²½ë¡œ
        """
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"YAML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
        
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        
        # YAML íŒŒì¼ ê²€ì¦
        required_keys = ['train', 'val', 'parts']
        for key in required_keys:
            if key not in data_config:
                raise ValueError(f"YAML íŒŒì¼ì— '{key}' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {yaml_path}")
        
        train_txt = data_config['train']
        val_txt = data_config['val']
        test_txt = data_config.get('test', None)
        parts = data_config['parts']
        mode = data_config.get('mode', None)
        preprocess_cfg = data_config.get('preprocess', 'on')

        # partsì— ë”°ë¥¸ í´ë˜ìŠ¤ êµ¬ì„±
        if isinstance(parts, str):
            parts_lower = parts.strip().lower()
        else:
            raise ValueError("YAMLì˜ 'parts' ê°’ì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: parts: frontdoor ë˜ëŠ” parts: bolt")

        self.parts = parts_lower
        self.mode = mode.strip().lower() if isinstance(mode, str) else None

        # preprocess on/off í•´ì„
        if isinstance(preprocess_cfg, str):
            preprocess_flag = preprocess_cfg.strip().lower() in ['on', 'true', '1', 'yes']
        elif isinstance(preprocess_cfg, bool):
            preprocess_flag = preprocess_cfg
        elif isinstance(preprocess_cfg, int):
            preprocess_flag = preprocess_cfg != 0
        else:
            preprocess_flag = True
        self.preprocess_enabled = preprocess_flag

        if parts_lower in ['frontdoor', 'door']:
            if self.mode == 'simple':
                # 1,2,3 -> 1 ë¡œ ë§¤í•‘í•˜ì—¬ 2-class í•™ìŠµ
                self.class_names = ['good', 'defect']
                self.label_map = {0: 0, 1: 1, 2: 1, 3: 1}
            else:
                self.class_names = ['good', 'no sealing', 'sealing differs', 'tape sealing']
                self.label_map = None
        elif parts_lower == 'bolt':
            self.class_names = ['good', 'bad']
            self.label_map = None
        else:
            raise ValueError("'parts' ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'frontdoor' ë˜ëŠ” 'bolt' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        self.num_classes = len(self.class_names)
        
        print(f"\nâœ“ ë°ì´í„° YAML íŒŒì¼ ë¡œë“œ: {yaml_path}")
        print(f"  - í•™ìŠµ ë°ì´í„°: {train_txt}")
        print(f"  - ê²€ì¦ ë°ì´í„°: {val_txt}")
        if test_txt:
            print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_txt}")
        print(f"  - parts: {parts} (í´ë˜ìŠ¤ ìˆ˜: {self.num_classes})")
        if self.mode:
            print(f"  - mode: {self.mode}")
        print(f"  - preprocess: {'on' if self.preprocess_enabled else 'off'}")
        
        # ë°ì´í„°ì…‹ í†µê³„
        with open(train_txt, 'r') as f:
            train_data = [line.strip() for line in f if line.strip()]
        with open(val_txt, 'r') as f:
            val_data = [line.strip() for line in f if line.strip()]
        
        # ë¼ë²¨ ìœ íš¨ì„± ê²€ì‚¬ ë° í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚°
        def parse_label(line):
            try:
                return int(line.split()[-1])
            except Exception:
                raise ValueError(f"ì˜ëª»ëœ ë¼ë²¨ í˜•ì‹: '{line}'. ê° ì¤„ì€ 'ì´ë¯¸ì§€ê²½ë¡œ ë¼ë²¨' ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        train_labels = [parse_label(line) for line in train_data]
        val_labels = [parse_label(line) for line in val_data]

        # door/frontdoor ì›ì²œ ë¼ë²¨ì€ 0~3, boltëŠ” 0~1 ë²”ìœ„ ê²€ì‚¬
        if self.parts in ['frontdoor', 'door']:
            max_allowed = 3
        elif self.parts == 'bolt':
            max_allowed = 1
        else:
            max_allowed = self.num_classes - 1

        invalid_train = [l for l in train_labels if l < 0 or l > max_allowed]
        invalid_val = [l for l in val_labels if l < 0 or l > max_allowed]
        if invalid_train or invalid_val:
            raise ValueError(
                f"ë¼ë²¨ ê°’ì´ 'parts={parts}'ì˜ ì›ì²œ ë¼ë²¨ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. "
                f"í—ˆìš© ë²”ìœ„: 0~{max_allowed}, "
                f"ì˜ëª»ëœ í•™ìŠµ ë¼ë²¨ ì˜ˆ: {invalid_train[:5]}, ì˜ëª»ëœ ê²€ì¦ ë¼ë²¨ ì˜ˆ: {invalid_val[:5]}"
            )

        # mode=simpleì´ë©´ ë§¤í•‘ í›„ í†µê³„ë¥¼ ìµœì¢… í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œ
        if self.label_map is not None:
            mapped_train = [self.label_map.get(l, l) for l in train_labels]
            mapped_val = [self.label_map.get(l, l) for l in val_labels]
            train_counts = [sum(1 for l in mapped_train if l == cid) for cid in range(self.num_classes)]
            val_counts = [sum(1 for l in mapped_val if l == cid) for cid in range(self.num_classes)]
        else:
            train_counts = [sum(1 for l in train_labels if l == cid) for cid in range(self.num_classes)]
            val_counts = [sum(1 for l in val_labels if l == cid) for cid in range(self.num_classes)]
        
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
        print(f"  - í•™ìŠµ ì´ë¯¸ì§€: {len(train_data)}ê°œ")
        print(f"  - ê²€ì¦ ì´ë¯¸ì§€: {len(val_data)}ê°œ")
        for cid, name in enumerate(self.class_names):
            print(f"    * {cid} - {name}: train {train_counts[cid]} / val {val_counts[cid]}")
        
        return train_txt, val_txt, test_txt
    
    def _get_transforms(self, is_train=True):
        """ë°ì´í„° augmentation ë° ì „ì²˜ë¦¬"""
        if not self.preprocess_enabled:
            return transforms.Compose([
                transforms.Resize((self.imgsz, self.imgsz)),
                transforms.ToTensor(),
            ])
        if is_train:
            return transforms.Compose([
                transforms.Resize((self.imgsz, self.imgsz)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.5),
                transforms.RandomRotation(15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            return transforms.Compose([
                transforms.Resize((self.imgsz, self.imgsz)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
    
    def train(self, train_txt, val_txt, test_txt=None):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            train_txt (str): í•™ìŠµ ë°ì´í„° txt íŒŒì¼ ê²½ë¡œ
            val_txt (str): ê²€ì¦ ë°ì´í„° txt íŒŒì¼ ê²½ë¡œ
            test_txt (str): í…ŒìŠ¤íŠ¸ ë°ì´í„° txt íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ DINOv2 í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}\n")
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_dinov2_model()
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        train_dataset = DefectDataset(
            train_txt,
            transform=self._get_transforms(is_train=True),
            label_map=self.label_map,
        )
        val_dataset = DefectDataset(
            val_txt,
            transform=self._get_transforms(is_train=False),
            label_map=self.label_map,
        )
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, 
                                 shuffle=True, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, 
                               shuffle=False, num_workers=4, pin_memory=True)
        
        print(f"ğŸ“Š ë°ì´í„°ë¡œë” ì¤€ë¹„ ì™„ë£Œ:")
        print(f"  - í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"  - ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}\n")
        
        best_val_acc = 0.0
        patience = 10
        patience_counter = 0
        
        # í•™ìŠµ ë£¨í”„
        last_val_true = []
        last_val_pred = []

        for epoch in range(self.epochs):
            print(f"\n{'='*60}")
            print(f"Epoch [{epoch+1}/{self.epochs}]")
            print(f"{'='*60}")

            # Epoch ì‹œì‘ ì‹œ freeze/unfreeze ì ìš©
            if epoch < self.freeze_epochs:
                # ë°±ë³¸ ê³ ì •, ë°±ë³¸ LR=0
                for p in self.model.backbone.parameters():
                    p.requires_grad = False
                if self._opt_group_backbone_idx is not None:
                    self.optimizer.param_groups[self._opt_group_backbone_idx]['lr'] = 0.0
            else:
                # ë°±ë³¸ í•´ì œ, ë°±ë³¸ LR ë³µì›
                for p in self.model.backbone.parameters():
                    p.requires_grad = True
                if self._opt_group_backbone_idx is not None:
                    self.optimizer.param_groups[self._opt_group_backbone_idx]['lr'] = self.lr_backbone
            
            # í•™ìŠµ ë‹¨ê³„
            train_loss, train_acc = self._train_epoch(train_loader)
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            
            # ê²€ì¦ ë‹¨ê³„
            val_loss, val_acc, val_true, val_pred = self._validate(val_loader)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)
            last_val_true = val_true
            last_val_pred = val_pred
            
            print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
            print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            
            # Best ëª¨ë¸ ì €ì¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                self._save_checkpoint('best.pt', epoch, val_acc)
                print(f"  âœ“ Best ëª¨ë¸ ì €ì¥! (Val Acc: {val_acc:.2f}%)")
            else:
                patience_counter += 1
            
            # Last ëª¨ë¸ ì €ì¥ (10 ì—í¬í¬ë§ˆë‹¤)
            if (epoch + 1) % 10 == 0:
                self._save_checkpoint('last.pt', epoch, val_acc)
            
            # Early stopping
            if patience_counter >= patience:
                print(f"\nâš ï¸  Early stopping triggered (patience: {patience})")
                break
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self._save_checkpoint('last.pt', epoch, val_acc)
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        self._save_results()
        self._plot_training_curves()

        # í˜¼ë™í–‰ë ¬ ì €ì¥ (ë§ˆì§€ë§‰ ê²€ì¦ ê¸°ì¤€)
        try:
            if last_val_true and last_val_pred and self.class_names:
                self._plot_confusion_matrix(last_val_true, last_val_pred)
        except Exception as e:
            print(f"í˜¼ë™í–‰ë ¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        print(f"\n{'='*60}")
        print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.save_dir}")
        print(f"  - weights/best.pt: ìµœì  ëª¨ë¸ (Val Acc: {best_val_acc:.2f}%)")
        print(f"  - weights/last.pt: ë§ˆì§€ë§‰ ëª¨ë¸")
        print(f"  - results.png: í•™ìŠµ ê³¡ì„ ")
        print(f"  - metrics.json: í•™ìŠµ ë©”íŠ¸ë¦­")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì˜ˆì¸¡ ë° ì´ë¯¸ì§€ ë¶„ë¥˜
        if test_txt and os.path.exists(test_txt):
            print(f"\n{'='*60}")
            print(f"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œì‘")
            print(f"{'='*60}\n")
            self._test_and_classify_images(test_txt)
    
    def _train_epoch(self, train_loader):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for images, labels in pbar:
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # Forward with AMP
            self.optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast('cuda', enabled=(self.device == 'cuda' and self.use_amp)):
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)

            # Backward with GradScaler
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # í†µê³„
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100.*correct/total:.2f}%'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy
    
    def _validate(self, val_loader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        all_true = []
        all_pred = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="Validation")
            for images, labels in pbar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                with torch.amp.autocast('cuda', enabled=(self.device == 'cuda' and self.use_amp)):
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

                all_true.extend(labels.detach().cpu().tolist())
                all_pred.extend(predicted.detach().cpu().tolist())
                
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100.*correct/total:.2f}%'
                })
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy, all_true, all_pred
    
    def _save_checkpoint(self, filename, epoch, val_acc):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        weights_dir = self.save_dir / 'weights'
        weights_dir.mkdir(exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_acc': val_acc,
            'config': {
                'project_name': self.project_name,
                'model_size': self.model_size,
                'imgsz': self.imgsz,
                'batch_size': self.batch_size,
                'lr': self.lr,
                'lr_head': self.lr_head,
                'lr_backbone': self.lr_backbone,
                'freeze_epochs': self.freeze_epochs,
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'parts': self.parts,
                'mode': self.mode,
                'preprocess': self.preprocess_enabled,
            }
        }
        
        torch.save(checkpoint, weights_dir / filename)
    
    def _save_results(self):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        metrics = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs,
            'best_val_acc': max(self.val_accs) if self.val_accs else 0,
            'config': {
                'project_name': self.project_name,
                'model_size': self.model_size,
                'imgsz': self.imgsz,
                'batch_size': self.batch_size,
                'epochs': self.epochs,
                'lr': self.lr,
                'lr_head': self.lr_head,
                'lr_backbone': self.lr_backbone,
                'freeze_epochs': self.freeze_epochs,
                'num_classes': self.num_classes,
                'class_names': self.class_names,
                'parts': self.parts,
                'mode': self.mode,
                'preprocess': self.preprocess_enabled,
            }
        }
        
        with open(self.save_dir / 'metrics.json', 'w') as f:
            json.dump(metrics, f, indent=4)
    
    def _plot_training_curves(self):
        """í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # Loss ê³¡ì„ 
        ax1.plot(epochs, self.train_losses, 'b-', label='Train Loss', linewidth=2)
        ax1.plot(epochs, self.val_losses, 'r-', label='Val Loss', linewidth=2)
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Accuracy ê³¡ì„ 
        ax2.plot(epochs, self.train_accs, 'b-', label='Train Acc', linewidth=2)
        ax2.plot(epochs, self.val_accs, 'r-', label='Val Acc', linewidth=2)
        ax2.set_xlabel('Epoch', fontsize=12)
        ax2.set_ylabel('Accuracy (%)', fontsize=12)
        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'results.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_confusion_matrix(self, y_true, y_pred):
        """í˜¼ë™í–‰ë ¬ íˆíŠ¸ë§µ ì €ì¥"""
        num_classes = self.num_classes if self.num_classes is not None else (max(max(y_true), max(y_pred)) + 1)

        # í˜¼ë™í–‰ë ¬ ê³„ì‚°
        cm = [[0 for _ in range(num_classes)] for _ in range(num_classes)]
        for t, p in zip(y_true, y_pred):
            if 0 <= t < num_classes and 0 <= p < num_classes:
                cm[t][p] += 1

        fig, ax = plt.subplots(figsize=(6 + num_classes * 0.4, 5 + num_classes * 0.3))
        im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
        ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

        # ì¶•/ë¼ë²¨
        class_names = self.class_names if self.class_names else [str(i) for i in range(num_classes)]
        ax.set(xticks=range(num_classes), yticks=range(num_classes),
               xticklabels=class_names, yticklabels=class_names,
               ylabel='True label', xlabel='Predicted label',
               title='Confusion Matrix (Validation)')

        # ë¼ë²¨ íšŒì „
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')

        # ì…€ ë‚´ ìˆ«ì ì£¼ì„
        thresh = max(max(row) for row in cm) * 0.5 if cm and cm[0] else 0
        for i in range(num_classes):
            for j in range(num_classes):
                value = cm[i][j]
                ax.text(j, i, str(value), ha='center', va='center',
                        color='white' if value > thresh else 'black')

        plt.tight_layout()
        out_path = self.save_dir / 'confusion_matrix.png'
        plt.savefig(out_path, dpi=300, bbox_inches='tight')
        plt.close()

        # ê°„ë‹¨í•œ ì •ë‹µ ê°œìˆ˜ ì¶œë ¥
        total_correct = sum(cm[i][i] for i in range(num_classes))
        total_samples = len(y_true)
        print(f"í˜¼ë™í–‰ë ¬ ì €ì¥: {out_path}")
        print(f"ê²€ì¦ ì •ë‹µ ê°œìˆ˜: {total_correct} / {total_samples} ({(100.0*total_correct/total_samples if total_samples else 0):.2f}%)")

    def _test_and_classify_images(self, test_txt):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° correct/incorrect í´ë”ì— ì´ë¯¸ì§€ ë¶„ë¥˜
        
        Args:
            test_txt (str): í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ì´ ë‹´ê¸´ txt íŒŒì¼
        """
        # í´ë” ìƒì„±
        correct_dir = self.save_dir / 'test_results' / 'correct'
        incorrect_dir = self.save_dir / 'test_results' / 'incorrect'
        correct_dir.mkdir(parents=True, exist_ok=True)
        incorrect_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        test_dataset = DefectDataset(
            test_txt,
            transform=self._get_transforms(is_train=False),
            label_map=self.label_map,
        )
        
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, 
                                shuffle=False, num_workers=4, pin_memory=True)
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ì¤€ë¹„ ì™„ë£Œ:")
        print(f"  - í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}")
        print(f"  - í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜: {len(test_dataset)}\n")
        
        self.model.eval()
        
        correct_count = 0
        incorrect_count = 0
        
        # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
        with open(test_txt, 'r') as f:
            test_lines = [line.strip() for line in f if line.strip()]
        
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(test_loader, desc="Testing")
            batch_idx = 0
            
            for images, labels in pbar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                with torch.amp.autocast('cuda', enabled=(self.device == 'cuda' and self.use_amp)):
                    outputs = self.model(images)
                
                _, predicted = outputs.max(1)
                
                # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ ì²˜ë¦¬
                for i in range(images.size(0)):
                    img_idx = batch_idx * self.batch_size + i
                    if img_idx >= len(test_lines):
                        break
                    
                    # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                    line_parts = test_lines[img_idx].split()
                    img_path = ' '.join(line_parts[:-1])
                    
                    true_label = labels[i].item()
                    pred_label = predicted[i].item()
                    confidence = torch.softmax(outputs[i], dim=0)
                    
                    all_predictions.append(pred_label)
                    all_labels.append(true_label)
                    
                    # ì´ë¯¸ì§€ ë³µì‚¬
                    if os.path.exists(img_path):
                        img_filename = os.path.basename(img_path)
                        
                        # íŒŒì¼ëª…ì— ì˜ˆì¸¡ ì •ë³´ ì¶”ê°€
                        name_parts = os.path.splitext(img_filename)
                        class_name_true = self.class_names[true_label] if true_label < len(self.class_names) else str(true_label)
                        class_name_pred = self.class_names[pred_label] if pred_label < len(self.class_names) else str(pred_label)
                        new_filename = f"{name_parts[0]}_gt{true_label}({class_name_true})_pred{pred_label}({class_name_pred})_conf{confidence[pred_label]:.2f}{name_parts[1]}"
                        
                        if true_label == pred_label:
                            dest_path = correct_dir / new_filename
                            shutil.copy2(img_path, dest_path)
                            correct_count += 1
                        else:
                            dest_path = incorrect_dir / new_filename
                            shutil.copy2(img_path, dest_path)
                            incorrect_count += 1
                
                batch_idx += 1
                pbar.set_postfix({
                    'correct': correct_count,
                    'incorrect': incorrect_count,
                    'acc': f'{100.*correct_count/(correct_count+incorrect_count):.2f}%' if (correct_count+incorrect_count) > 0 else '0%'
                })
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µê³„
        total = correct_count + incorrect_count
        accuracy = 100. * correct_count / total if total > 0 else 0
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µê³„")
        print(f"{'='*60}")
        print(f"  - ì´ ì´ë¯¸ì§€: {total}")
        print(f"  - ì •ë‹µ: {correct_count}")
        print(f"  - ì˜¤ë‹µ: {incorrect_count}")
        print(f"  - ì •í™•ë„: {accuracy:.2f}%")
        print(f"\nğŸ“ ë¶„ë¥˜ëœ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜:")
        print(f"  - Correct: {correct_dir}")
        print(f"  - Incorrect: {incorrect_dir}")
        
        # í…ŒìŠ¤íŠ¸ í˜¼ë™í–‰ë ¬ ìƒì„±
        if len(all_labels) > 0:
            self._plot_test_confusion_matrix(all_labels, all_predictions)
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON ì €ì¥
        test_results = {
            'total': total,
            'correct': correct_count,
            'incorrect': incorrect_count,
            'accuracy': accuracy,
            'class_names': self.class_names
        }
        
        with open(self.save_dir / 'test_results.json', 'w') as f:
            json.dump(test_results, f, indent=4)
        
        print(f"âœ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {self.save_dir / 'test_results.json'}")

    def _plot_test_confusion_matrix(self, y_true, y_pred):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜¼ë™í–‰ë ¬ ì €ì¥"""
        num_classes = self.num_classes if self.num_classes is not None else (max(max(y_true), max(y_pred)) + 1)

        # í˜¼ë™í–‰ë ¬ ê³„ì‚°
        cm = [[0 for _ in range(num_classes)] for _ in range(num_classes)]
        for t, p in zip(y_true, y_pred):
            if 0 <= t < num_classes and 0 <= p < num_classes:
                cm[t][p] += 1

        fig, ax = plt.subplots(figsize=(6 + num_classes * 0.4, 5 + num_classes * 0.3))
        im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
        ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

        # ì¶•/ë¼ë²¨
        class_names = self.class_names if self.class_names else [str(i) for i in range(num_classes)]
        ax.set(xticks=range(num_classes), yticks=range(num_classes),
               xticklabels=class_names, yticklabels=class_names,
               ylabel='True label', xlabel='Predicted label',
               title='Confusion Matrix (Test)')

        # ë¼ë²¨ íšŒì „
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')

        # ì…€ ë‚´ ìˆ«ì ì£¼ì„
        thresh = max(max(row) for row in cm) * 0.5 if cm and cm[0] else 0
        for i in range(num_classes):
            for j in range(num_classes):
                value = cm[i][j]
                ax.text(j, i, str(value), ha='center', va='center',
                        color='white' if value > thresh else 'black')

        plt.tight_layout()
        out_path = self.save_dir / 'test_confusion_matrix.png'
        plt.savefig(out_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ“ í…ŒìŠ¤íŠ¸ í˜¼ë™í–‰ë ¬ ì €ì¥: {out_path}")


def main():
    parser = argparse.ArgumentParser(description='DINOv2 ì–‘í’ˆ/ë¶ˆëŸ‰ ë¶„ë¥˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸')
    
    parser.add_argument('--project', type=str, required=True,
                        help='í”„ë¡œì íŠ¸ ì´ë¦„')
    parser.add_argument('--data-yaml', type=str, required=True,
                        help='ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model-size', type=str, default='small',
                        choices=['small', 'base', 'large', 'giant'],
                        help='DINOv2 ëª¨ë¸ í¬ê¸° (ê¸°ë³¸ê°’: small)')
    parser.add_argument('--imgsz', type=int, default=224,
                        help='ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 224)')
    parser.add_argument('--batch', type=int, default=32,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 100)')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 1e-4)')
    parser.add_argument('--lr-backbone', type=float, default=None,
                        help='ë°±ë³¸ í•™ìŠµë¥  (ê¸°ë³¸ì€ lrì˜ 0.1ë°°)')
    parser.add_argument('--lr-head', type=float, default=None,
                        help='ë¶„ë¥˜ê¸° í—¤ë“œ í•™ìŠµë¥  (ê¸°ë³¸ì€ lr)')
    parser.add_argument('--freeze-epochs', type=int, default=0,
                        help='ì´ˆê¸° ë°±ë³¸ ê³ ì • ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ 0)')
    parser.add_argument('--device', type=str, default='cuda',
                        choices=['cuda', 'cpu'],
                        help='ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: cuda)')
    
    args = parser.parse_args()
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = DINOv2Trainer(
        project_name=args.project,
        model_size=args.model_size,
        imgsz=args.imgsz,
        batch_size=args.batch,
        epochs=args.epochs,
        lr=args.lr,
        device=args.device,
        lr_backbone=args.lr_backbone,
        lr_head=args.lr_head,
        freeze_epochs=args.freeze_epochs
    )
    
    # ë°ì´í„° ë¡œë“œ
    train_txt, val_txt, test_txt = trainer.load_data_yaml(args.data_yaml)
    
    # í•™ìŠµ ì‹œì‘
    trainer.train(train_txt, val_txt, test_txt)


if __name__ == "__main__":
    # ì˜ˆì‹œ 1: ê¸°ë³¸ í•™ìŠµ (test ë°ì´í„° ìˆëŠ” ê²½ìš°)
    # python train_dinov2.py --project defect_detection --data-yaml data/defect.yaml
    
    # ì˜ˆì‹œ 2: Large ëª¨ë¸ ì‚¬ìš©
    # python train_dinov2.py --project defect_detection --data-yaml data/defect.yaml --model-size large
    
    # ì˜ˆì‹œ 3: ì»¤ìŠ¤í…€ ì„¤ì •
    # python train_dinov2.py --project defect_detection --data-yaml data/defect.yaml --model-size base --imgsz 384 --batch 16 --epochs 200 --lr 5e-5
    
    # ì˜ˆì‹œ 4: ì½”ë“œì—ì„œ ì§ì ‘ ì‹¤í–‰
    # trainer = DINOv2Trainer(project_name='defect_detection', model_size='base', epochs=150)
    # train_txt, val_txt, test_txt = trainer.load_data_yaml('data/defect.yaml')
    # trainer.train(train_txt, val_txt, test_txt)
    
    main()